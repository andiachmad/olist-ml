{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales Forecasting (Univariate Tabular-TS) - Spark 2.4.4\n",
    "# Pipeline: \n",
    "# load -> log1p -> features(lag/MA/seasonality) -> TS split -> Model Search (GBT vs RF, small TS-CV) \n",
    "# -> Train Final -> Evaluate (log & original scale) -> Plots\n",
    "\n",
    "# Notes: gw pake limiter biar friendly di VM 6GB, out of memory error kalo kg \n",
    "# (shuffle kecil, grid kecil, toPandas hanya untuk plot)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "\n",
    "# --- non-interactive matplotlib (biar bisa save gambar di VM/headless)\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inisialisasi, spark session n configs\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DM_GBT_RF_TabularTS_2.4.4\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"8\")  # kecilkan shuffle untuk VM 6GB\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngeload data, di path sama jadi tinggal load nama aja. \n",
    "# sesuain dir klo beda e.g. \"C:\\Users\\user\\Desktop\\AOL_Data Mining\\attempt1\\data.csv\"\n",
    "df0 = (spark.read.format(\"csv\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .option(\"inferSchema\", \"true\")\n",
    "       .load(\"sales_data.csv\"))\n",
    "\n",
    "# normalisasiin nama kolom jdi lowercase\n",
    "df0 = df0.select([F.col(c).alias(c.lower()) for c in df0.columns])\n",
    "\n",
    "required = [\"week_start\", \"total_sales\"]\n",
    "missing = [c for c in required if c not in df0.columns]\n",
    "if missing:\n",
    "    raise ValueError(\"Kolom wajib tidak ditemukan: %s\" % \", \".join(missing))\n",
    "\n",
    "# parse tanggal, agregasi per minggu (jaga2 kalo ad duplikat baris per mingguny juga)\n",
    "df = (df0\n",
    "      .withColumn(\"week_start_dt\", F.to_date(F.col(\"week_start\")))\n",
    "      .withColumn(\"total_sales\", F.col(\"total_sales\").cast(DoubleType()))\n",
    "      .withColumn(\"total_sales\", F.when(F.col(\"total_sales\") < 0, 0.0).otherwise(F.col(\"total_sales\")))\n",
    "      .groupBy(\"week_start_dt\").agg(F.sum(\"total_sales\").alias(\"total_sales\"))\n",
    "      .orderBy(\"week_start_dt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target transform (log)\n",
    "# log1p ngejaga nilai 0, jadi label dasar.\n",
    "df = df.withColumn(\"y_log\", F.log1p(F.col(\"total_sales\")))\n",
    "\n",
    "# gw pake winsorize (cap) di log scale biar bs nekenin effect si outlier waktu training\n",
    "# ambil perkiraan quantile 0.99 (robust, cepat)\n",
    "cap_q = df.select(\"y_log\").na.drop().stat.approxQuantile(\"y_log\", [0.99], 0.01)[0]\n",
    "df = df.withColumn(\"y_log_train\", F.when(F.col(\"y_log\") > cap_q, cap_q).otherwise(F.col(\"y_log\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering, e.g :\n",
    "# - lags pendek & musiman\n",
    "# - moving averages (smoothing)\n",
    "# - seasonality encoding via sin/cos (mingguan ~ 52)\n",
    "# note lagi: semua fitur berbasis y_log (bukan original) biar stabil\n",
    "w = Window.orderBy(\"week_start_dt\")\n",
    "\n",
    "# Lags: tambahkan beberapa yg dekat + musiman\n",
    "lags = [1, 2, 3, 4, 8, 12, 26, 52]\n",
    "for k in lags:\n",
    "    df = df.withColumn(f\"lag_{k}\", F.lag(\"y_log\", k).over(w))\n",
    "\n",
    "# Moving average (window: baris, bukan kalender)\n",
    "df = df.withColumn(\"ma_4\",  F.avg(\"y_log\").over(Window.orderBy(\"week_start_dt\").rowsBetween(-3, 0)))\n",
    "df = df.withColumn(\"ma_8\",  F.avg(\"y_log\").over(Window.orderBy(\"week_start_dt\").rowsBetween(-7, 0)))\n",
    "df = df.withColumn(\"ma_12\", F.avg(\"y_log\").over(Window.orderBy(\"week_start_dt\").rowsBetween(-11, 0)))\n",
    "\n",
    "# differencing (menangkap perubahan)\n",
    "df = df.withColumn(\"diff_1\", F.col(\"y_log\") - F.col(\"lag_1\"))\n",
    "df = df.withColumn(\"diff_52\", F.col(\"y_log\") - F.col(\"lag_52\"))\n",
    "\n",
    "# seasonality features\n",
    "df = df.withColumn(\"woy\", F.weekofyear(\"week_start_dt\").cast(\"double\"))\n",
    "angle = F.col(\"woy\") * (2.0 * math.pi / 52.0)\n",
    "df = df.withColumn(\"sin52\", F.sin(angle)).withColumn(\"cos52\", F.cos(angle))\n",
    "\n",
    "# drop baris awal yg g punya lag/MA\n",
    "feature_cols = [f\"lag_{k}\" for k in lags] + [\"ma_4\", \"ma_8\", \"ma_12\", \"diff_1\", \"diff_52\", \"sin52\", \"cos52\"]\n",
    "df_feat = df.dropna(subset=feature_cols + [\"y_log\", \"y_log_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME-BASED SPLIT 80:20\n",
    "# Train = 80% pertama waktu; Test = 20% terakhir\n",
    "w_all = Window.orderBy(\"week_start_dt\")\n",
    "df_feat = df_feat.withColumn(\"rn\", F.row_number().over(w_all))\n",
    "n = df_feat.count()\n",
    "train_n = int(n * 0.8)\n",
    "train = df_feat.filter(F.col(\"rn\") <= train_n).cache()\n",
    "test  = df_feat.filter(F.col(\"rn\") >  train_n).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALL TIME-SERIES cross validator (grid ringan)\n",
    "# gw bikin manual, cv bawaan spark anomaly, ngelakuin shuffle k-fold (ga pas di case ini utk ts)\n",
    "# bikin 3 fold berurutan: train<=cut1 validate=cut1->cut2, dst.\n",
    "def make_ts_folds(df_in, folds=3):\n",
    "    \"\"\"bagi data training jadi beberapa blok waktu, untuk validasi berurutan.\"\"\"\n",
    "    total = df_in.count()\n",
    "    idxs = [int(total*(i+1)/(folds+1)) for i in range(folds)]  # ex: 25%, 50%, 75% dari train\n",
    "    cut_dates = [df_in.orderBy(\"week_start_dt\").limit(k).agg(F.max(\"week_start_dt\").alias(\"d\")).collect()[0][\"d\"] for k in idxs]\n",
    "    # Bentuk pasangan (train<=prev_cut, valid di (prev_cut, cut])\n",
    "    prev = None\n",
    "    splits = []\n",
    "    for d in cut_dates:\n",
    "        if prev is None:\n",
    "            tr = df_in.filter(F.col(\"week_start_dt\") <= d)\n",
    "        else:\n",
    "            tr = df_in.filter(F.col(\"week_start_dt\") <= prev)\n",
    "        va = df_in.filter((F.col(\"week_start_dt\") > prev) if prev is not None else (F.col(\"week_start_dt\") > F.lit(\"0001-01-01\"))) \\\n",
    "                  .filter(F.col(\"week_start_dt\") <= d)\n",
    "        splits.append((tr.cache(), va.cache()))\n",
    "        prev = d\n",
    "    return splits\n",
    "\n",
    "ts_splits = make_ts_folds(train, folds=3)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Dua kandidat model: GBT & RF\n",
    "def make_gbt(**kw):\n",
    "    return GBTRegressor(featuresCol=\"features\", labelCol=\"y_log_train\",\n",
    "                        maxIter=kw.get(\"maxIter\",150),\n",
    "                        maxDepth=kw.get(\"maxDepth\",8),\n",
    "                        stepSize=kw.get(\"stepSize\",0.1),\n",
    "                        subsamplingRate=kw.get(\"subsamplingRate\",0.8),\n",
    "                        seed=42)\n",
    "\n",
    "def make_rf(**kw):\n",
    "    return RandomForestRegressor(featuresCol=\"features\", labelCol=\"y_log_train\",\n",
    "                                 numTrees=kw.get(\"numTrees\",300),\n",
    "                                 maxDepth=kw.get(\"maxDepth\",12),\n",
    "                                 maxBins=kw.get(\"maxBins\",64),\n",
    "                                 subsamplingRate=kw.get(\"subsamplingRate\",0.8),\n",
    "                                 seed=42)\n",
    "\n",
    "# Grid kecil (biar hemat, gw di vm)\n",
    "gbt_grid = [\n",
    "    {\"model\":\"gbt\", \"maxIter\":100, \"maxDepth\":6, \"stepSize\":0.10},\n",
    "    {\"model\":\"gbt\", \"maxIter\":150, \"maxDepth\":8, \"stepSize\":0.05},\n",
    "]\n",
    "rf_grid = [\n",
    "    {\"model\":\"rf\", \"numTrees\":300, \"maxDepth\":10},\n",
    "    {\"model\":\"rf\", \"numTrees\":500, \"maxDepth\":12},\n",
    "]\n",
    "param_grid = gbt_grid + rf_grid\n",
    "\n",
    "def rmse_log(df_pred):\n",
    "    ev = RegressionEvaluator(labelCol=\"y_log\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    return ev.evaluate(df_pred)\n",
    "\n",
    "best_cfg, best_score = None, float(\"inf\")\n",
    "for cfg in param_grid:\n",
    "    # build model\n",
    "    if cfg[\"model\"] == \"gbt\":\n",
    "        reg = make_gbt(**cfg)\n",
    "    else:\n",
    "        reg = make_rf(**cfg)\n",
    "    pipe = Pipeline(stages=[assembler, reg])\n",
    "\n",
    "    # avg RMSE(log) across TS folds\n",
    "    rmses = []\n",
    "    for tr, va in ts_splits:\n",
    "        m = pipe.fit(tr)\n",
    "        pred_va = m.transform(va)\n",
    "        score = rmse_log(pred_va)  # di log scale (stabil vs outlier)\n",
    "        rmses.append(score)\n",
    "    avg_rmse = sum(rmses) / len(rmses)\n",
    "    print(\"CFG %s -> avg RMSE(log)=%.4f\" % (cfg, avg_rmse))\n",
    "\n",
    "    if avg_rmse < best_score:\n",
    "        best_score, best_cfg = avg_rmse, cfg\n",
    "\n",
    "print(\"\\n>> best config (TimeSeries CrossValid, RMSE log):\", best_cfg, \"score=\", round(best_score,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final training dataset (pake best config)\n",
    "reg_final = make_gbt(**best_cfg) if best_cfg[\"model\"]==\"gbt\" else make_rf(**best_cfg)\n",
    "pipe_final = Pipeline(stages=[assembler, reg_final])\n",
    "model = pipe_final.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediksi + invers transform\n",
    "pred = (model.transform(test)\n",
    "        .withColumn(\"pred_log\", F.col(\"prediction\"))\n",
    "        .withColumn(\"y_pred\", F.expm1(F.col(\"pred_log\")))   # balik ke skala asli\n",
    "        .withColumn(\"y_true\", F.col(\"total_sales\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluasi, original & log scale\n",
    "\n",
    "# a) Original scale\n",
    "pred_eval = pred.select(\"y_true\",\"y_pred\").dropna()\n",
    "\n",
    "rmse = (pred_eval\n",
    "        .withColumn(\"se\", (F.col(\"y_true\")-F.col(\"y_pred\"))**2)\n",
    "        .agg(F.sqrt(F.avg(\"se\")).alias(\"rmse\"))\n",
    "        .collect()[0][\"rmse\"])\n",
    "\n",
    "mae = (pred_eval\n",
    "       .withColumn(\"ae\", F.abs(F.col(\"y_true\")-F.col(\"y_pred\")))\n",
    "       .agg(F.avg(\"ae\").alias(\"mae\"))\n",
    "       .collect()[0][\"mae\"])\n",
    "\n",
    "# MAPE ny terfilter (ignore minggu dengan true terlalu kecil, biar ga eksplosif)\n",
    "mape_floor = 1000.0\n",
    "mape = (pred_eval\n",
    "        .withColumn(\"ape\", F.when(F.col(\"y_true\") >= mape_floor,\n",
    "                                  F.abs((F.col(\"y_true\")-F.col(\"y_pred\"))/F.col(\"y_true\"))))\n",
    "        .agg(F.avg(\"ape\").alias(\"mape\"))\n",
    "        .collect()[0][\"mape\"])\n",
    "\n",
    "# sMAPE (more stabil)\n",
    "smape = (pred_eval\n",
    "         .withColumn(\"sm\", (F.abs(F.col(\"y_pred\")-F.col(\"y_true\")) /\n",
    "                            ((F.abs(F.col(\"y_true\"))+F.abs(F.col(\"y_pred\")))/2.0)))\n",
    "         .agg(F.avg(\"sm\").alias(\"smape\")).collect()[0][\"smape\"])\n",
    "\n",
    "mean_true = pred_eval.agg(F.avg(\"y_true\").alias(\"m\")).collect()[0][\"m\"]\n",
    "ss_tot = pred_eval.withColumn(\"d\", (F.col(\"y_true\")-mean_true)**2).agg(F.sum(\"d\")).collect()[0][0]\n",
    "ss_res = pred_eval.withColumn(\"r\", (F.col(\"y_true\")-F.col(\"y_pred\"))**2).agg(F.sum(\"r\")).collect()[0][0]\n",
    "r2 = float(1.0 - (ss_res/ss_tot)) if (ss_tot is not None and ss_tot != 0) else None\n",
    "\n",
    "print(\"\\n=== TEST SET METRICS (ORIGINAL SCALE) ===\")\n",
    "print(\"RMSE: %.4f | MAE: %.4f | MAPE(>=%.0f): %s | sMAPE: %s | R^2: %s\" %\n",
    "      (rmse, mae, mape_floor,\n",
    "       \"NA\" if mape is None else \"{:.2f}%\".format(mape*100.0),\n",
    "       \"NA\" if smape is None else \"{:.2f}%\".format(smape*100.0),\n",
    "       \"NA\" if r2 is None else \"{:.4f}\".format(r2)))\n",
    "\n",
    "# b) Log scale (fair untuk data skewed)\n",
    "pred_log_eval = (model.transform(test)\n",
    "                 .select(F.col(\"y_log\").alias(\"y_true_log\"),\n",
    "                         F.col(\"prediction\").alias(\"y_pred_log\"))\n",
    "                 .dropna())\n",
    "\n",
    "ev_rmse_log = RegressionEvaluator(labelCol=\"y_true_log\", predictionCol=\"y_pred_log\", metricName=\"rmse\")\n",
    "ev_mae_log  = RegressionEvaluator(labelCol=\"y_true_log\", predictionCol=\"y_pred_log\", metricName=\"mae\")\n",
    "rmse_log = ev_rmse_log.evaluate(pred_log_eval)\n",
    "mae_log  = ev_mae_log.evaluate(pred_log_eval)\n",
    "\n",
    "# R2 log (manual)\n",
    "mean_log = pred_log_eval.agg(F.avg(\"y_true_log\").alias(\"m\")).collect()[0][\"m\"]\n",
    "ss_tot_log = pred_log_eval.withColumn(\"d\", (F.col(\"y_true_log\")-mean_log)**2).agg(F.sum(\"d\")).collect()[0][0]\n",
    "ss_res_log = pred_log_eval.withColumn(\"r\", (F.col(\"y_true_log\")-F.col(\"y_pred_log\"))**2).agg(F.sum(\"r\")).collect()[0][0]\n",
    "r2_log = float(1.0 - (ss_res_log/ss_tot_log)) if (ss_tot_log is not None and ss_tot_log != 0) else None\n",
    "\n",
    "print(\"\\n=== TEST SET METRICS (LOG SCALE) ===\")\n",
    "print(\"RMSE_log: %.4f | MAE_log: %.4f | R^2_log: %s\" %\n",
    "      (rmse_log, mae_log, \"NA\" if r2_log is None else \"{:.4f}\".format(r2_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display important feature/insight biar bs diambil (top 12)\n",
    "last_stage = model.stages[-1]\n",
    "importances = last_stage.featureImportances.toArray()\n",
    "fi = sorted(list(zip(feature_cols, importances)), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop feature importances:\")\n",
    "for name, score in fi[:12]:\n",
    "    print(\"{:<10s} : {:.4f}\".format(name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot gw save di dir home cloudera, biar gambar bs gw copy n save di real laptop\n",
    "# distribusi & boxplot (original vs log)\n",
    "pdf_all = df.select(\"total_sales\",\"y_log\").dropna().toPandas()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(pdf_all[\"total_sales\"], bins=50)\n",
    "plt.title(\"Distribution of total_sales (original)\"); plt.xlabel(\"total_sales\"); plt.ylabel(\"count\")\n",
    "plt.tight_layout(); plt.savefig(\"dist_original.png\", dpi=150); plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(pdf_all[\"total_sales\"].values, vert=True)\n",
    "plt.title(\"Boxplot of total_sales (original)\")\n",
    "plt.tight_layout(); plt.savefig(\"box_original.png\", dpi=150); plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(pdf_all[\"y_log\"], bins=50)\n",
    "plt.title(\"Distribution of log1p(total_sales)\"); plt.xlabel(\"log1p(total_sales)\"); plt.ylabel(\"count\")\n",
    "plt.tight_layout(); plt.savefig(\"dist_log.png\", dpi=150); plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(pdf_all[\"y_log\"].values, vert=True)\n",
    "plt.title(\"Boxplot of log1p(total_sales)\")\n",
    "plt.tight_layout(); plt.savefig(\"box_log.png\", dpi=150); plt.close()\n",
    "\n",
    "# actual vs predicted (line) + scatter (test set)\n",
    "pdf_test = (pred.select(\"week_start_dt\",\"y_true\",\"y_pred\").orderBy(\"week_start_dt\").toPandas())\n",
    "plt.figure()\n",
    "plt.plot(pdf_test[\"week_start_dt\"], pdf_test[\"y_true\"], label=\"Actual\")\n",
    "plt.plot(pdf_test[\"week_start_dt\"], pdf_test[\"y_pred\"], label=\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted (Test, original scale)\")\n",
    "plt.xlabel(\"week_start\"); plt.ylabel(\"total_sales\"); plt.legend(); plt.xticks(rotation=45)\n",
    "plt.tight_layout(); plt.savefig(\"actual_vs_pred.png\", dpi=150); plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(pdf_test[\"y_true\"], pdf_test[\"y_pred\"])\n",
    "maxv = max(pdf_test[\"y_true\"].max(), pdf_test[\"y_pred\"].max())\n",
    "plt.plot([0, maxv],[0, maxv])\n",
    "plt.title(\"Actual vs Predicted (Scatter, Test)\")\n",
    "plt.xlabel(\"Actual total_sales\"); plt.ylabel(\"Predicted total_sales\")\n",
    "plt.tight_layout(); plt.savefig(\"scatter_ap.png\", dpi=150); plt.close()\n",
    "\n",
    "print(\"\\nSaved plots: dist_original.png, box_original.png, dist_log.png, box_log.png, actual_vs_pred.png, scatter_ap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head of predictions, biar bs sanity check di log & original\n",
    "pred.select(\"week_start_dt\",\"y_true\",\"y_pred\",\"pred_log\").orderBy(\"week_start_dt\").show(12, truncate=False)\n",
    "\n",
    "# (optional yg ini) unpersist\n",
    "train.unpersist(); test.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
